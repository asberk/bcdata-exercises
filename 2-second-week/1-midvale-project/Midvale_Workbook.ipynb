{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/asberk/data/1-Donaldson/AllData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkColumn(df, colNum):\n",
    "    \"\"\"\n",
    "    Used in throwAwayUnchanged\n",
    "    \"\"\"\n",
    "    return np.all(df.iloc[0, colNum] == df.iloc[1:, colNum])\n",
    "\n",
    "\n",
    "def throwAwayUnchanged(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    Could, however, prove useful on subsetted-by-group data...\n",
    "    This function throws away columns that are the same in every entry\n",
    "    \"\"\"\n",
    "    idxUnhelpful = [j for j in range(df.columns.size)\n",
    "                    if checkColumn(df, j)]\n",
    "    df = df.drop(df.columns[idxUnhelpful], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def throwAwayBizarre(df):\n",
    "    \"\"\"\n",
    "    Throws away rows where TotalBytes is negative \n",
    "    (because this doesn't make sense).\n",
    "    \"\"\"\n",
    "    df = df.loc[df['TotalBytes'] >= 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def removeUnwanted(data):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    # Don't worry about High Performance mode for this task\n",
    "    data = data.groupby('Mode').get_group(0)\n",
    "    # Flicker is not useful for prediction\n",
    "    data = data.drop('Flicker', axis=1)\n",
    "    # We will throw away columns that are all the same\n",
    "    # (On `data`, this gets rid of Sharpening, \n",
    "    #  WaitSeconds and Status)\n",
    "    data = throwAwayUnchanged(data)\n",
    "    data = throwAwayBizarre(data)\n",
    "    data = data.drop('Index', axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fixMiscValues(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project. \n",
    "    \"\"\"\n",
    "    df = df.fillna({'TertiaryResolution': 'NaN'})\n",
    "    df = df.replace('-', value=0)\n",
    "    df['SecondaryBitsPerSecond'] = df['SecondaryBitsPerSecond'].astype(np.float64)\n",
    "    df['TertiaryBitsPerSecond'] = df['TertiaryBitsPerSecond'].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preProcess(df):\n",
    "    \"\"\"\n",
    "    Made specifically for the data we were given for the Midvale project\n",
    "    \"\"\"\n",
    "    df = removeUnwanted(df)\n",
    "    df = fixMiscValues(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preProcess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a subset of the data for a simpler time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to figure out the subset...\n",
    "\n",
    "Roger recommended sticking with `Test == Base` and a single camera. Let's choose the camera with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_gb_CameraName = data.loc[data['Test']=='Base'].groupby(['CameraName'])\n",
    "CameraName_highestCountOf_Base = Base_gb_CameraName.count()['Test'].argmax()\n",
    "data_A3Base = Base_gb_CameraName.get_group(CameraName_highestCountOf_Base)\n",
    "data_A3Base = data_A3Base.drop(['CameraName', 'Test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_A3Base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_A3Base['TotalBytes'].values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_A3Base['PrimaryBitsPerSecond'].values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_A3Base['SecondaryBitsPerSecond'].values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logTransformColumns(df, columns):\n",
    "    logDict = {'log' + col: lambda x: np.log(x[col]) \n",
    "               for col in columns}\n",
    "    df = df.assign(**logDict)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(logTotalBytes = lambda x: np.log(x.TotalBytes))\n",
    "data = data.assign(logPrimaryBPS = lambda x: np.log(x.PrimaryBitsPerSecond) \n",
    "                   if x.PrimaryBitsPerSecond > 0 else 0)\n",
    "data = data.assign(logSecondaryBPS = lambda x: np.log(x.SecondaryBitsPerSecond) \n",
    "                   if x.SecondaryBitsPerSecond > 0 else 0)\n",
    "data = data.assign(logTertiaryBPS = lambda x: np.log(x.TertiaryBitsPerSecond) \n",
    "                   if x.TertiaryBitsPerSecond > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = logTransformColumns(data, ['TotalBytes', 'PrimaryBitsPerSecond', \n",
    "                                  'SecondaryBitsPerSecond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_colVals(X, **kwargs):\n",
    "    \"\"\"\n",
    "    X : a categorical column of a data frame\n",
    "    \"\"\"\n",
    "    # Check if not categorical\n",
    "    #\n",
    "    #\n",
    "    # get value counts\n",
    "    vc = X.value_counts()\n",
    "    n = vc.shape[0]\n",
    "    xrange = np.arange(n)\n",
    "    plt.bar(xrange, vc.values, **kwargs)\n",
    "    plt.xticks(xrange, vc.index.tolist(), rotation=90)\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of these\n",
    "# (these are the *useful* categories for CamerName:A3;Test:Base)\n",
    "categs = ['PrimaryResolution', 'SecondaryResolution', \n",
    "          'Keyframe', 'ImageRate', 'Quality',\n",
    "          'Detail', 'Motion']\n",
    "\n",
    "C = len(categs)\n",
    "ncols = 4\n",
    "nrows = np.int(np.ceil(C/5))\n",
    "figwidth = 20\n",
    "figheight = 20 #np.int(np.min([np.ceil(20/ncols*nrows), 20]))\n",
    "\n",
    "plt.subplots(nrows, ncols, figsize=(figwidth, figheight))\n",
    "for j, categ in enumerate(categs):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    hist_colVals(data_BaseA3[categ])\n",
    "    plt.xlabel(categ)\n",
    "for j in range(C, nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, j+1)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting out `TotalBytes` and some categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_data_BaseA3 = data_BaseA3.sort_values(by='TotalBytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data_BaseA3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_data_BaseA3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding correlations with PrimaryBitsPerSecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_BaseA3['PrimaryBitsPerSecond']), bins=1000, cumulative=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_BaseA3 = data_BaseA3.assign(logPrimaryBitsPerSecond = lambda x: np.log(x.PrimaryBitsPerSecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "qualityResponse = scaler.fit_transform(data_BaseA3.loc[:, ['Quality', 'logPrimaryBitsPerSecond']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist2d(qualityResponse[:,0],\n",
    "           qualityResponse[:,1],\n",
    "           bins=20);\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('log(PrimaryBitsPerSecond)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_BaseA3.loc[:,['Quality', 'ImageRate', 'Keyframe']].values,\n",
    "                                                    data_BaseA3['logPrimaryBitsPerSecond'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = ElasticNetCV(normalize=True)\n",
    "en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist2d(data_BaseA3['Quality'], \n",
    "           np.log(data_BaseA3['PrimaryBitsPerSecond']),\n",
    "           bins=20);\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('log(PrimaryBitsPerSecond)');\n",
    "qual = np.arange(0, 21)\n",
    "logPBPS_pred = en.predict(qual.reshape(-1,1))\n",
    "plt.plot(qual, logPBPS_pred, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whaddaya know, (log) bit rate is correlated with -Quality..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features for regression\n",
    "\n",
    "## Encoding the categoricals\n",
    "\n",
    "If there are any categoricals, then maybe we should put columns as integers so we can regress on them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setUpCategs(data, sparse=False):\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    lb = LabelEncoder()\n",
    "    oh = OneHotEncoder()\n",
    "    \n",
    "    categoricals = data.select_dtypes(include=['object'])\n",
    "    categoricals = pd.concat((categoricals, data['Nonlinear']), axis=1)\n",
    "    categoricals = categoricals.drop('Message', axis=1)\n",
    "    categoricals = categoricals.apply(lb.fit_transform)\n",
    "    categoricals = oh.fit_transform(categoricals)\n",
    "    if not sparse:\n",
    "        categoricals = categoricals.toarray()\n",
    "    return categoricals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricals = setUpCategs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continuous = data.loc[:, ['Keyframe', 'ImageRate', 'Quality', 'KbpsLimit', 'CollectSeconds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the response(s)\n",
    "TotalBytes should never be negative so far as I'm aware, so let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['TotalBytes'] = data['TotalBytes'].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_names = ['TotalBytes', 'PrimaryBitsPerSecond', 'SecondaryBitsPerSecond', 'TertiaryBitsPerSecond']\n",
    "responses = data.filter(items=response_names).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.log(responses[:,0]), stacked=True);\n",
    "plt.legend(response_names[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testingCorrelation = data.loc[:,['TotalBytes', 'PrimaryBitsPerSecond']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.abs(testingCorrelation[:,0])), np.log(testingCorrelation[:,1]), '.', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohCameraName = oh.fit_transform(lb.fit_transform(data['CameraName'].values.ravel()).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohCameraName.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_encode = ['CameraName', 'PrimaryResolution', 'SecondaryResolution', \n",
    "                      'Nonlinear', 'Mode', 'Test', 'Detail', 'Motion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ftrName in features_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    data[ftrName+'Enc'] = le.fit_transform(data[ftrName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "* We really want to predict `log(PrimaryBitsPerSecond)`. And we probably want to scale it first. \n",
    "* What are the other variables we want to scale? \n",
    "* Are we allowed to use Quality in our prediction? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
