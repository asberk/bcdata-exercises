{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split in decision trees? \n",
    "\n",
    "In every spltiting step we maximize the information gain using entropy. \n",
    "\n",
    "Let $X$ be a discrete random variable with possible values $\\{x_1, x_2, \\ldots, x_n\\}$ and probability function $\\mathbb P (X)$. Then the entropy is \n",
    "$$\n",
    "H(X) := -\\sum_{i=1}^n P(X = x_i) \\log(\\mathbb P(X = x_i))\n",
    "$$\n",
    "\n",
    "Let $m$ be the number of samples in a node and $k$ be the number of output classes. For each $i = 1, \\ldots, k$, $m_i$ samples of the node belong to class $i$. Then the entropy of this node is \n",
    "$$\n",
    "H = - \\sum_{i=1}^k \\frac{m_i}{m} \\log \\big(\\frac{m_i}{m}\\big)\n",
    "$$\n",
    "\n",
    "Let $n$ be the number of features. For each feature $F \\in \\{ 1, \\ldots, n\\}$ we can perform a split of the node. The feature $F$ can have $f_F$ possible values. \n",
    "\n",
    "For each $i = 1, \\ldots, f_F$ the number $m_i^F$ is the number of samples, part of the parent node that accept the value $i$ as feature $F$. \n",
    "\n",
    "Then \n",
    "$$\n",
    "\\sum_{i=1}^{f_F} \\frac{m_i^F}{m} H_i \n",
    "$$\n",
    "is the entropy of the split. \n",
    "\n",
    "For each feature $F \\in \\{1, \\ldots, n\\}$, we calculate the entropy of the split. Finally, we split wrt feature \n",
    "$$\n",
    "\\underset{F\\in\\{1, \\ldots, n\\}}{\\arg\\min} \\sum_{i=1}^{f_F} \\frac{m_i^F}{m} H_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Can also split using Gini index instead of Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to stop splitting\n",
    "\n",
    "What can happen when we split:\n",
    "\n",
    "1. Perfect split (Entropy 0).\n",
    "2. No samples anymore, or conflicting case\n",
    "3. No splitting brings improvement in entropy. Predict majority class. \n",
    "\n",
    "## Pruning\n",
    "\n",
    "Pruning is a method to prevent overfitting. check using a $\\chi^2$ test if the split is significant or just by chance.\n",
    "\n",
    "## Ensemble methods\n",
    "\n",
    "\"Two heads are smarter than one\"\n",
    "\n",
    "We improve fitting by combining models. Combination of models are generally known as **model ensembles**. \n",
    "\n",
    "Pros:\n",
    "\n",
    "* Average measurements can lead to more stable and reliable estimates because we reduce the influence of random fluctuations of single samples\n",
    "* By combining different models, the models overcome the shortcomings of a single model\n",
    "\n",
    "### How to achieve diversity\n",
    "\n",
    "* Use only a subset of train data\n",
    "* Use only a subset of features\n",
    "* Use different **methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Let $T$ be the number of models. To train:  \n",
    "```\n",
    "for t = 1, ..., T\n",
    "    * Draw random subset with replacement from train data\n",
    "    * Fit a model M_t with the drawn train samples\n",
    "```\n",
    "\n",
    "For each test sample:  \n",
    "```\n",
    "for t = 1, ..., T\n",
    "    Use model M_t to predict outcome\n",
    "    Take average/vote of all predictions\n",
    "```\n",
    "\n",
    "The models `M_t` are usually from the same method. Bagging is used for all kinds of methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "For training a random forest:\n",
    "```\n",
    "Fix T, the number of models to train.\n",
    "for t = 1, ..., T:\n",
    "    * Draw a random subset of the train data (with replacement)\n",
    "    * Draw a random subset of features (without replacement) of size Ã±\n",
    "    * Fit model M_t with drawn train data and features\n",
    "```\n",
    "\n",
    "**Note:** random forests are only used for decision trees. \n",
    "\n",
    "**Note:** the testing step works the same way as for Bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
